node_modules/

# Local LLM infrastructure (see .claude/skills/fastqwen.md for setup)
llama.cpp/
*.gguf
